{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projektvorstellung Natural Language Processing\n",
    "\n",
    "&copy; Daniel Schaudt, [Prof. Dr. Reinhold von Schwerin](https://www.thu.de/Reinhold.vonSchwerin), Technische Hochschule Ulm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook soll aufgezeigt werden, wie mit Deep Learning-Methoden ein einfacher Chatbot erstellt werden kann, welcher Fragen zu bestimmten Themen beantwortet. Dieser Bereich des Machine Learning lässt sich dem *Natural Language Processing (NLP)* zuordnen - also der Verarbeitung von Sprache, die von Menschen benutzt wird. Dieses Notebook orientiert sich an einem Blogpost von [Dirk Hornung](https://chatbotslife.com/how-to-build-a-chatbot-from-zero-a0ebb186b070).\n",
    "\n",
    "Der verwendete Technologie-Stack besteht im Kern aus `Python` und der Bibliothek `Tensorflow`. Alle weiteren Bibliotheken dienen als Helfer für die Datenverarbeitung. Entgegen der vorherrschenden Meinung, dass für Deep Learning besonders viele Daten benötigt werden, kommt diese Demonstration mit nur wenigen Daten aus (4 KB Textfile). Dadurch soll gezeigt werden, dass auch das eigenständige Sammeln und *labeln* von Daten in einem Deep Learning-Projekt zum Erfolg führen kann.\n",
    "\n",
    "**Anmerkung:** Deep Learning ist ein komplexes Thema! Das Ziel dieses Notebooks ist es, anhand eines einfachen Anwendungsfalles einen groben Überblick über Deep Learning-Methoden zu geben, welcher von Lesern mit unterschiedlichem Hintergrund verstanden werden kann. Für interessierte Leser stellt dieses Notebook aber immer wieder Literaturreferenzen bereit, welche das Thema detaillierter aufarbeiten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Datenprojekte strukturiert aufzubauen, bietet sich der Einsatz eines etablierten Vorgehensmodells an. Diese Projektvorstellung orientiert sich am [CRISP-DM-Modell](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining). Dieses Modell deckt die folgenden Projekt-Phasen ab:\n",
    "\n",
    "![CRISP-DM](./images/Cross-Industry-Standard-Process-for-Data-Mining-CRISP-DM-12_s.png \"CRISP-DM\")\n",
    "\n",
    "\n",
    "Um den Aufbau verständlich zu halten, entsprechen die Überschriften dieses Notebooks genau den Phasen des CRISP-DM-Prozesses. Die Kurzbeschreibung der Phasen in diesem Notebook entstammen aus [Pete Chapman et al](ftp://ftp.software.ibm.com/software/analytics/spss/support/Modeler/Documentation/14/UserManual/CRISP-DM.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Aus CRISP-DM 1.0:** *Diese erste Phase konzentriert sich auf das Verständnis der Projektziele und -anforderungen aus der Geschäftsperspektive. Dieses Wissen wird dann in eine Data-Mining-Problemdefinition und einen vorläufigen Plan umgewandelt, um\n",
    "die Ziele zu erreichen.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laut [IBM 2017](https://www.ibm.com/blogs/watson/2017/10/how-chatbots-reduce-customer-service-costs-by-30-percent/) geben Unternehmen jedes 1,3 Billionen Dollar für Kundenservice-Anrufe aus. Wenn Chatbots in der Lage sind auch nur einen geringen Teil der 265 Milliarden Anrufe obsolet zu machen, dann sind gigantische Einsparungen zu erwarten. Der in diesem Notebook vorgestellte, *sehr einfache* Chatbot könnte wahrscheinlich nur einen Teil des First-Level-Supports übernehmen - diese Routineanfragen machen laut IBM aber 80% aller Anfragen aus.\n",
    "\n",
    "Konkret soll hier ein Chatbot für Anfragen an ein *medizinisches Enterprise-System* vorgestellt werden. Der Chatbot soll vor allem neue Nutzer schnell mit Informationen versorgen und (in der implementierten Variante) auch einfache API-Aufrufe absetzen können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Aus CRISP-DM 1.0:** *Die Phase Data Understanding beginnt mit der ersten Datensammlung und geht weiter mit Aktivitäten, die es ermöglichen, sich mit den Daten vertraut machen. Dazu gehören: Datenqualitätsprobleme zu identifizieren, erste Einblicke in die Daten zu gewinnen und/oder interessante Teilmengen zu entdecken, um Hypothesen über verborgene Informationen zu bilden.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduzierbarkeit erreichen\n",
    "\n",
    "Setzen diverser [Random Seeds](https://de.wikipedia.org/wiki/Seed_key) für determistisches Trainingsverhalten (d.h. zur Reproduzierbarkeit der Ergebnisse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSEED = 42\n",
    "import random as rand\n",
    "rand.seed(RSEED)\n",
    "from numpy.random import seed\n",
    "seed(RSEED)\n",
    "from tensorflow import random\n",
    "random.set_seed(RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bemerkung** Leider erreicht man damit u.U. nur eine *lokale Reproduzierbarkeit*, d.h. die Ergebnisse auf einem bestimmten Rechner bleiben bei auch sonst unveränderten Modellparametern (s.u.) gleich. Rechnerübergreifend kann aber zum Tragen kommen, was [Jason Brownlee](https://machinelearningmastery.com/reproducible-results-neural-networks-keras/) im Abschnitt *Randomness from a Sophisticated Model* schreibt. Ohnehin sollte man Brownlees Empfehlung folgen und zur Beurteilung der Performance des Modells dieses mehrfach ausführen.\n",
    "\n",
    "Im Falle des doch recht einfachen Modells, welches hier generiert wird, ist aber offenbar die Reproduzierbarkeit über verschiedene Rechner hinweg gegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import der Bibliotheken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachfolgend werden die benötigten Bibliotheken importiert. Eine kurze Information zu den wichtigsten Bibliotheken:\n",
    "* [Tensorflow](https://www.tensorflow.org/): Open-source Machine Learning-Bibliothek, welche den Kern dieses Notebooks bildet. Mit Tensorflow werden die Deep Learning-Modelle erzeugt und trainiert. Genauer gesagt wird in diesem Notebook die High-Level API [Keras](https://www.tensorflow.org/guide/keras/overview) verwendet, welche seit Tensorflow 2.0 fest integriert ist. Auch für die Vorverarbeitung der Daten wird die Bibliothek benutzt.\n",
    "* [numpy](https://numpy.org/): Eine mächtige Bibliothek für wissenschaftliche Berechnungen. Wir verwenden in diesem Notebook die Array-Funktionalitäten von numpy.\n",
    "* [pandas](https://pandas.pydata.org/): Mächtiges Werkzeug zur Verarbeitung tabellarischer Daten in Python. Wird hier lediglich für die optisch ansprechende Ausgabe der Testergebnisse verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grundlegendes zum Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein einfacher, wie in diesem Notebook vorgestellter *Contextual Chatbot* versucht die Intention einer Anfrage zu verstehen. Im Grunde versucht der Chatbot also ein *Mapping* eines *Inputs* auf eine *Absicht (Intent)* zu erlernen: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Chatbot Basics](./images/chatbot.png \"Chatbot Basics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bild von [Dirk Hornung](https://chatbotslife.com/how-to-build-a-chatbot-from-zero-a0ebb186b070)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn die Absicht erkannt wurde, dann soll eine vordefinierte Antwort ausgelöst werden. Es wäre aber auch denkbar externe Datenquellen als Antwort anzuzapfen. Mit diesem Wissen ist der Aufbau der Trainingsdaten besser zu verstehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden der Textdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Die Textdaten stammen aus Dataflairs [Chatbot Projekt](https://data-flair.training/blogs/python-chatbot-project/), wobei die Daten ursprünglich vermutlich von [Andrej Baranovskij](https://andrejusb.blogspot.com/2018/03/classification-machine-learning-chatbot.html) übernommen wurden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten liegen im `json`-Format vor und befinden sich im Ordner `./data`. Mit `json.load()` können diese wie folgt geladen werden: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/intents_health.json', 'r') as f:\n",
    "    intents = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das resultierende Objekt ist ein Python [Dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) mit folgendem Inhalt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'greeting',\n",
       "   'patterns': ['Hi there',\n",
       "    'How are you',\n",
       "    'Is anyone there?',\n",
       "    'Hey',\n",
       "    'Hola',\n",
       "    'Hello',\n",
       "    'Good day'],\n",
       "   'responses': ['Hello, thanks for asking',\n",
       "    'Good to see you again',\n",
       "    'Hi there, how can I help?']},\n",
       "  {'tag': 'goodbye',\n",
       "   'patterns': ['Bye',\n",
       "    'See you later',\n",
       "    'Goodbye',\n",
       "    'Nice chatting to you, bye',\n",
       "    'Till next time'],\n",
       "   'responses': ['See you!', 'Have a nice day', 'Bye! Come back again soon.']},\n",
       "  {'tag': 'thanks',\n",
       "   'patterns': ['Thanks',\n",
       "    'Thank you',\n",
       "    \"That's helpful\",\n",
       "    'Awesome, thanks',\n",
       "    'Thanks for helping me'],\n",
       "   'responses': ['Happy to help!', 'Any time!', 'My pleasure']},\n",
       "  {'tag': 'noanswer',\n",
       "   'patterns': [],\n",
       "   'responses': [\"Sorry, can't understand you\",\n",
       "    'Please give me more info',\n",
       "    'Not sure I understand']},\n",
       "  {'tag': 'options',\n",
       "   'patterns': ['How you could help me?',\n",
       "    'What you can do?',\n",
       "    'What help you provide?',\n",
       "    'How you can be helpful?',\n",
       "    'What support is offered'],\n",
       "   'responses': ['I can guide you through Adverse drug reaction list, Blood pressure tracking, Hospitals and Pharmacies',\n",
       "    'Offering support for Adverse drug reaction, Blood pressure, Hospitals and Pharmacies']},\n",
       "  {'tag': 'adverse_drug',\n",
       "   'patterns': ['How to check Adverse drug reaction?',\n",
       "    'Open adverse drugs module',\n",
       "    'Give me a list of drugs causing adverse behavior',\n",
       "    'List all drugs suitable for patient with adverse reaction',\n",
       "    'Which drugs dont have adverse reaction?'],\n",
       "   'responses': ['Navigating to Adverse drug reaction module']},\n",
       "  {'tag': 'blood_pressure',\n",
       "   'patterns': ['Open blood pressure module',\n",
       "    'Task related to blood pressure',\n",
       "    'Blood pressure data entry',\n",
       "    'I want to log blood pressure results',\n",
       "    'Blood pressure data management'],\n",
       "   'responses': ['Navigating to Blood Pressure module']},\n",
       "  {'tag': 'blood_pressure_search',\n",
       "   'patterns': ['I want to search for blood pressure result history',\n",
       "    'Blood pressure for patient',\n",
       "    'Load patient blood pressure result',\n",
       "    'Show blood pressure results for patient',\n",
       "    'Find blood pressure results by ID'],\n",
       "   'responses': ['Please provide Patient ID', 'Patient ID?']},\n",
       "  {'tag': 'pharmacy_search',\n",
       "   'patterns': ['Find me a pharmacy',\n",
       "    'Find pharmacy',\n",
       "    'List of pharmacies nearby',\n",
       "    'Locate pharmacy',\n",
       "    'Search pharmacy'],\n",
       "   'responses': ['Please provide pharmacy name']},\n",
       "  {'tag': 'hospital_search',\n",
       "   'patterns': ['Lookup for hospital',\n",
       "    'Searching for hospital to transfer patient',\n",
       "    'I want to search hospital data',\n",
       "    'Hospital lookup for patient',\n",
       "    'Looking up hospital details'],\n",
       "   'responses': ['Please provide hospital name or location']}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Datei enthält folgende Elemente:\n",
    "* **Tag:** Entspricht dem *Label* oder *Intent* einer Nachricht, also der Absicht einer Anfrage.\n",
    "* **Patterns:** Beispielhafte *Anfragemuster*, welche möglicherweise im Zusammenhang mit einem *Tag* auftreten.\n",
    "* **Responses:** Mindestens eine Antwortmöglichkeiten für den Chatbot.\n",
    "\n",
    "Weiterhin ist anzumerken, dass die Datei wirklich nicht groß ist. Sie enthält lediglich 10 Tags und nur wenige Patterns. Die Anzahl der Patterns pro Tag sollten zudem ausgewogen sein, da das Modell sonst dazu neigt, jene Tags mit den meisten Patterns auszuwählen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Aus CRISP-DM 1.0:** *Die Phase Data Preparation umfasst alle Aktivitäten, die zur Erstellung des finalen Datensatzes aus den anfänglichen Rohdaten benötigt werden. Datenvorbereitungsaufgaben werden unter Umständen mehrfach und nicht\n",
    "in vorgegebener Reihenfolge ausgeführt. Zu den Aufgaben gehören die Auswahl von Tabellen, Datensätzen und Attributen sowie die Transformation und Bereinigung von Daten für die Modellierung.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Input eines Machine Learning-Modells muss numerisch sein - mit den reinen Wörtern (strings) kann das Modell leider nicht arbeiten. Die Sätze müssen also als eine Sequenz von Zahlen dargestellt werden. Dieser Prozess, bei welchem jedem Wort eine einzigartige ID zugeordnet wird, nennt sich *Tokenization* und ist im Grunde nichts anderes als ein Mapping. Zu diesem Zweck wird das Vokabular der Patterns zunächst in eine Liste `intent_list` umgewandelt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_list = []\n",
    "label_list = []\n",
    "for index, intent in enumerate(intents['intents']):\n",
    "    intent_list += intent['patterns']\n",
    "    num_patterns = len(intent['patterns'])\n",
    "    label_list += [index] * num_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ebenso wird eine `label_list` erzeugt, welche jedem Pattern den zugehörigen Tag zuordnet. Diese wird später für den Trainingsprozess benötigt, damit das Modell die korrekte Zuordnung der Patterns zu den Tags erlernt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend findet der Tokenization-Prozess statt. Es wird der Tokenizer aus `keras.preprocessing` verwendet und auf die `intent_list` angepasst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_tok = '<OOV>' #Out-of-Vocabulary Token\n",
    "tokenizer = Tokenizer(oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(intent_list)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das `oov_tok` entspricht einem *Out-of-Vocabulary-Token*, also einem Platzhalter für unbekannte Wörter, die in dem *Textkorpus* der Trainingsdaten nicht vorkommen. Das Modell soll schließlich auch alle Eingaben erkennen, welche *ähnlich* zu den festgelegten Patterns sind. Würde es lediglich die genauen Patterns erkennen, dann bräuchten wir kein Machine Learning, sondern eine Tabelle!\n",
    "\n",
    "In `word_index` sind nun alle Wörter mit einer ID als Key-Value-Paar in einem [Python-Dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) gespeichert. Die ersten 5 Wörter sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1, 'blood': 2, 'pressure': 3, 'you': 4, 'for': 5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(word_index.items())[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können die einzelnen Patterns in Zahlenfolgen umgewandelt werden. Dies geschieht bequem mit der `texts_to_sequences`-Funktion eines angepassten tokenizer-Objekts. Beispielhaft hier die ersten 10 Sätze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[36, 24],\n",
       " [10, 37, 4],\n",
       " [25, 38, 24],\n",
       " [39],\n",
       " [40],\n",
       " [41],\n",
       " [42, 43],\n",
       " [26],\n",
       " [44, 4, 45],\n",
       " [46]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(intent_list); sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Input des hier gezeigten neuronalen Netzes muss immer die gleich Länge aufweisen. Da die Sätze aber unterschiedlich lang sind, müssen diese *aufgefüllt* werden. Dieser Prozess wird als *Padding* bezeichnet. Konkret wird nach dem längsten Satz gesucht und die restlichen Sätze dann auf die gleiche Länge mit Nullen aufgefüllt. Nähere Informationen zu Padding gibt es u.a. in diesem [Blogpost](https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(sequences)\n",
    "padded_length = len(padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier sieht man das Padding in einem Ausschnitt, welcher die längsten Sätze enthält:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0, 10,  4, 29, 59, 27],\n",
       "       [ 0,  0,  0,  0,  0, 15, 60, 25, 61],\n",
       "       [ 0,  0,  0, 10,  6, 62,  8, 63, 16],\n",
       "       [ 0,  0,  0,  0,  0, 30,  8, 12, 31],\n",
       "       [64, 11, 32, 17, 33, 12, 65,  8, 66],\n",
       "       [17, 67, 12, 68,  5,  7, 69,  8, 16],\n",
       "       [ 0,  0,  0, 70, 12, 71, 72,  8, 16],\n",
       "       [ 0,  0,  0,  0,  0, 30,  2,  3, 31],\n",
       "       [ 0,  0,  0,  0, 73, 74,  6,  2,  3],\n",
       "       [ 0,  0,  0,  0,  0,  2,  3, 18, 75]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Größe des Vokabulars wird noch definiert, da diese für das Deep Learning-Modell später wichtig ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der aktuellen Struktur der Daten ist jedem Pattern ein Tag zugeordnet, welches mit einer Zahl (0-9 für 10 verschiedene Tags) kodiert ist. Damit das Deep Learning-Modell später aber eine Vorhersage treffen kann, eignet sich eine Darstellung der Labels als *One-Hot-Encoding*. Jedes Label wird dadurch zu einem binären *On-Off-Switch*. Mehr Informationen zu One-Hot-Encoding gibt es u.a. auf [Hackernoon](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f). Am einfachsten kann man sich das an den Daten verdeutlichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = tf.keras.utils.to_categorical(label_list); labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Länge der Labels entspricht nun also der Anzahl an Tags. Die Zugehörigkeit zu einem Tag wird mit einer 1 (*Hot*) repräsentiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abschließend wird noch die Anzahl an Kategorien definiert, da diese für die Größe des Outputs des Deep Learning-Modells relevant sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = len(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Aus CRISP-DM 1.0:** *In dieser Phase werden verschiedene Modellierungstechniken ausgewählt und angewendet und ihre Parameter auf optimale Werte kalibriert. Normalerweise gibt es mehrere Techniken für denselben Data-Mining-Problemtyp. Einige Techniken haben spezifische Anforderungen an die Form der Daten. Daher ist es oft notwendig, zur Datenvorbereitungsphase zurückzugehen.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird das Deep Learning-Modell definiert und trainiert. Für NLP-Aufgaben können eine Reihe von verschiedenartigen neuronalen Netzen zum Einsatz kommen. Im Rahmen dieser Projektvorstellung wird ein einfaches *Feedforward-Netzwerk* verwendet. Für komplexere Aufgaben werden jedoch häufig *Recurrent Neural Networks (RNN)* oder *Transformer Networks* eingesetzt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weitere Informationen zu neuronalen Netzen im Allgemeinen und modernen NLP-Varianten im Speziellen finden sich u.a. in folgender Literatur:\n",
    "* Exzellente Videoeinführung zu NNs von 3Blue1Brown, unbedingt [anschauen!](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "* Grundlegendes Konzept von neuronalen Netzen mit interaktiven Beispielen und Visualisierungen: [Jay Alammar](https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)\n",
    "* Ebenfalls auf [Jay Alammar's Blog](http://jalammar.github.io/) finden sich gelungene Visualisierungen zu aktuellen NLP-Architekturen wie BERT, GPT-2 und weiteren Seq2Seq-Modellen.\n",
    "* Interessantes Video zu Googles aktueller Forschung im Bereich Chatbots: [Two Minute Papers](https://www.youtube.com/watch?v=3Wppf_CNvD0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die einzelnen Ebenen (*Layer*) des Modells werden nachfolgend definiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size+1, 16, input_length=padded_length),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_categories, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell hat gegenüber einem einfachen Feedforward-Netzwerk allerdings eine Besonderheit: [Word Embeddings](https://machinelearningmastery.com/what-are-word-embeddings/)! Durch diese wird jedes Wort in einen n-dimensionalen Vektorraum projiziert. Das bedeutet ein Wort wird nicht nur durch *eine* Zahl repräsentiert, sondern durch mehrere. In diesem Fall werden 16 Dimensionen verwendet. Dies hat im Kern zwei Vorteile: \n",
    "\n",
    "1. Ein Wort kann durch mehrere Features repräsentiert werden. Als Mensch könnte man sich folgende Features vorstellen: Gefühlslage, Sprachniveau, Alter des Wortes, etc. Welche \"Features\" das Modell nun genau erkennt, kann leider nicht nachvollzogen werden.\n",
    "2. Die Platzierung des Wortes in einem Vektorraum ermöglicht die Berechnung mit *Distanzen*. So können *ähnliche* Wörter dicht zusammen liegen und unähnliche weit voneinander entfernt. Eine interaktive 3D Visualisierung eines solchen Wort-Vektorraumes gibt es z.B. [hier](http://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachfolgend werden Trainingsparameter für das Modell bestimmt. Das Training entspricht einer Anpassung der Gewichte des Netzwerks mithilfe des stochastischen Gradientenverfahrens. Umfassende Erklärungen des Verfahrens sprengen leider den Rahmen dieser Einführung. Für weitere Informationen zum Training eines neuronalen Netzes wird daher auf [Nielsen](http://neuralnetworksanddeeplearning.com/chap2.html) verwiesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Zusammenfassung des Modells erhält man mit `summary()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 9, 16)             1456      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               18560     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 28,922\n",
      "Trainable params: 28,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Zusammenfassung zeigt die Architektur des Netzwerks, also die Layer sowie deren Art von Beginn des Netzwerks (Inputlayer) bis zum Ende (Outputlayer). Weiterhin kann die Größe der Layer und deren Verbindungen untersucht werden. Folgende Parameter des Modells werden noch definiert:\n",
    "\n",
    "* **Batchsize:** Die Batchsize gibt an, wieviele Sätze in einem *Durchgang* durch das neuronale Netz geleitet werden. Dieser Parameter ist im Wesentlichen aus zwei Gründen wichtig: 1) zur Speicheroptimierung und 2) für das Training des Modells. Die Wahl der Batchsize kann ein heikles Thema darstellen und hängt von vielen Faktoren ab. Eine Einführung geben u.a. [Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap2.html) oder Kapitel 8.1.3 von [Ian Goodfellow et al.](https://www.deeplearningbook.org/contents/optimization.html).\n",
    "* **Epochs:** Eine Epoch entspricht einem kompletten Durchlauf aller Trainingsdaten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abschließend folgt nun das Training des Modells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47 samples\n",
      "Epoch 1/25\n",
      "47/47 - 1s - loss: 2.3018 - acc: 0.0638\n",
      "Epoch 2/25\n",
      "47/47 - 0s - loss: 2.2842 - acc: 0.1702\n",
      "Epoch 3/25\n",
      "47/47 - 0s - loss: 2.2599 - acc: 0.3830\n",
      "Epoch 4/25\n",
      "47/47 - 0s - loss: 2.2381 - acc: 0.3404\n",
      "Epoch 5/25\n",
      "47/47 - 0s - loss: 2.2160 - acc: 0.3830\n",
      "Epoch 6/25\n",
      "47/47 - 0s - loss: 2.1628 - acc: 0.4468\n",
      "Epoch 7/25\n",
      "47/47 - 0s - loss: 2.0792 - acc: 0.5319\n",
      "Epoch 8/25\n",
      "47/47 - 0s - loss: 2.0055 - acc: 0.5106\n",
      "Epoch 9/25\n",
      "47/47 - 0s - loss: 1.8513 - acc: 0.5106\n",
      "Epoch 10/25\n",
      "47/47 - 0s - loss: 1.7100 - acc: 0.5745\n",
      "Epoch 11/25\n",
      "47/47 - 0s - loss: 1.5415 - acc: 0.5532\n",
      "Epoch 12/25\n",
      "47/47 - 0s - loss: 1.3260 - acc: 0.7021\n",
      "Epoch 13/25\n",
      "47/47 - 0s - loss: 1.2000 - acc: 0.6596\n",
      "Epoch 14/25\n",
      "47/47 - 0s - loss: 1.0661 - acc: 0.7021\n",
      "Epoch 15/25\n",
      "47/47 - 0s - loss: 0.9734 - acc: 0.7447\n",
      "Epoch 16/25\n",
      "47/47 - 0s - loss: 0.8236 - acc: 0.8298\n",
      "Epoch 17/25\n",
      "47/47 - 0s - loss: 0.7038 - acc: 0.8511\n",
      "Epoch 18/25\n",
      "47/47 - 0s - loss: 0.5729 - acc: 0.8723\n",
      "Epoch 19/25\n",
      "47/47 - 0s - loss: 0.6210 - acc: 0.8085\n",
      "Epoch 20/25\n",
      "47/47 - 0s - loss: 0.4905 - acc: 0.9574\n",
      "Epoch 21/25\n",
      "47/47 - 0s - loss: 0.4224 - acc: 0.9362\n",
      "Epoch 22/25\n",
      "47/47 - 0s - loss: 0.3597 - acc: 0.8936\n",
      "Epoch 23/25\n",
      "47/47 - 0s - loss: 0.3259 - acc: 0.9362\n",
      "Epoch 24/25\n",
      "47/47 - 0s - loss: 0.2793 - acc: 0.9787\n",
      "Epoch 25/25\n",
      "47/47 - 0s - loss: 0.1903 - acc: 0.9787\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded, labels, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Aus CRISP-DM 1.0:** *In dieser Phase des Projekts wurde ein Modell erstellt, welches eine ausreichende Performance zu haben scheint. Bevor das Modell endgültig implementiert wird, ist es wichtig, das Modell, sowie die Schritte, die zur Erstellung des Modells geführt haben, gründlich zu evaluieren und zu überprüfen, um sicher zu sein, dass das Modell die Geschäftsziele ordnungsgemäß erreicht. Es soll sichergestellt werden, dass alle Geschäftsziele ausreichend berücksichtigt wurden. Am Ende dieser Phase sollte eine Entscheidung über die Verwendung der Data-Mining-Ergebnisse getroffen werden.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird die Performance des Modells untersucht. **Es soll betont werden, dass es sich hierbei um eine *empirische Untersuchung* handelt** - es werden also Testsätze *ausprobiert* um ein Gefühl für die Performance des Modells zu bekommen. Diese Art der Evaluation ist der Datenknappheit geschuldet! In der Realität sollte das Modell anhand von Testdaten untersucht werden und eine statistische Auswertung von etablierten Metriken erfolgen. Die Testdaten sind derart zu wählen, dass das Modell diese **nicht** während des Trainingslaufs schon gesehen hat - sonst kann keine valide Beurteilung vorgenommen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie oben aus der Trainingshistorie ausgelesen werden kann, hat das Modell nach 25 Epochen (Trainingsdurchläufen) eine Genauigkeit von 97,87% erzielt. Das bedeutet, das Modell ordnet jedem Pattern in den Trainingsdaten zu 97,87% den zugehörigen *Tag* korrekt zu. Wie schon angemerkt, hat diese Angabe aber nur einen begrenzten Nutzen für die Anwendung in der Praxis! Es könnte ja sein, dass das Modell nur die Trainingsdaten *auswendig* lernt und nicht in der Lage ist auf Abweichungen entsprechend zu reagieren. Das soll nun mit einigen Beispielsätzen ausprobiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu diesem Zweck wird nachfolgend eine Funktion definiert, welche zu einem Input-Satz den vorhergesagten Tag zurückgibt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tag(sentence):\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=padded_length)\n",
    "    prediction = model.predict(padded_sequence)[0]\n",
    "    intent = intents['intents'][np.argmax(prediction)]\n",
    "    tag = intent['tag']\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Erinnerung, folgende Tags sind im Datensatz vorhanden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greeting\n",
      "goodbye\n",
      "thanks\n",
      "noanswer\n",
      "options\n",
      "adverse_drug\n",
      "blood_pressure\n",
      "blood_pressure_search\n",
      "pharmacy_search\n",
      "hospital_search\n"
     ]
    }
   ],
   "source": [
    "tag_list = []\n",
    "for tags in intents['intents']:\n",
    "    tag_list.append(tags['tag'])\n",
    "    print(tags['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es wird zu jedem Tag ein Beispielsatz entworfen, welcher **nicht** in dieser Form in den Trainingsdaten vorliegt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = ['Greetings',\n",
    "                 'It was nice meeting you',\n",
    "                 'Thank you very much',\n",
    "                 '',\n",
    "                 'How can you support me?',\n",
    "                 'What adverse reaction has this drug?',\n",
    "                 'I want to enter blood pressure',\n",
    "                 'What is the blood pressure for patient?',\n",
    "                 'Where is the neares pharmacy?',\n",
    "                 'I need data from another hospital']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Testsätze können natürlich beliebig einfach (ähnlich) oder schwer (unähnlich) gewählt werden. Dies macht die Beurteilung des Modells nicht leichter. In der Praxis werden Chatbots daher oft im laufenden Betrieb stetig verbessert und dazu Feedback der Benutzer gesammelt (\"Hat Ihnen diese Antwort geholfen?\").\n",
    "\n",
    "Die Sätze werden zusammen mit dem tatsächlichen Tag und der Vorhersage in einem `pandas`-DataFrame dargestellt. Das DataFrame dient nur der sauberen Darstellung als Tabelle im Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>True Tag</th>\n",
       "      <th>Predicted Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Greetings</td>\n",
       "      <td>greeting</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was nice meeting you</td>\n",
       "      <td>goodbye</td>\n",
       "      <td>blood_pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you very much</td>\n",
       "      <td>thanks</td>\n",
       "      <td>options</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>noanswer</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can you support me?</td>\n",
       "      <td>options</td>\n",
       "      <td>options</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What adverse reaction has this drug?</td>\n",
       "      <td>adverse_drug</td>\n",
       "      <td>adverse_drug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I want to enter blood pressure</td>\n",
       "      <td>blood_pressure</td>\n",
       "      <td>blood_pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the blood pressure for patient?</td>\n",
       "      <td>blood_pressure_search</td>\n",
       "      <td>blood_pressure_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Where is the neares pharmacy?</td>\n",
       "      <td>pharmacy_search</td>\n",
       "      <td>pharmacy_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I need data from another hospital</td>\n",
       "      <td>hospital_search</td>\n",
       "      <td>hospital_search</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Sentence               True Tag  \\\n",
       "0                                Greetings               greeting   \n",
       "1                  It was nice meeting you                goodbye   \n",
       "2                      Thank you very much                 thanks   \n",
       "3                                                        noanswer   \n",
       "4                  How can you support me?                options   \n",
       "5     What adverse reaction has this drug?           adverse_drug   \n",
       "6           I want to enter blood pressure         blood_pressure   \n",
       "7  What is the blood pressure for patient?  blood_pressure_search   \n",
       "8            Where is the neares pharmacy?        pharmacy_search   \n",
       "9        I need data from another hospital        hospital_search   \n",
       "\n",
       "           Predicted Tag  \n",
       "0               greeting  \n",
       "1         blood_pressure  \n",
       "2                options  \n",
       "3               greeting  \n",
       "4                options  \n",
       "5           adverse_drug  \n",
       "6         blood_pressure  \n",
       "7  blood_pressure_search  \n",
       "8        pharmacy_search  \n",
       "9        hospital_search  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = list(zip(test_sentences, tag_list, list(map(predict_tag, test_sentences))))\n",
    "pd.DataFrame(data_list, columns=['Sentence', 'True Tag', 'Predicted Tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Von den 10 unbekannten Sätzen hat das Modell also 7 richtig klassifiziert. Diese Testgenauigkeit (70%) liegt unter der Trainingsgenauigkeit. Im ML-Kontext spricht man hier von *Overfitting*. Das Modell hat eine Überanpassung an die Trainingsdaten vollzogen und verliert in Folge die Fähigkeit eine generalisierte Aussage für unbekannte Daten zu treffen. In der Praxis muss dieses Phänomen mit weiteren Tests genau unter die Lupe genommen werden. Weitere Informationen zur *Kapazität* eines Deep Learning-Modells finden sich Kapitel 5.2 von [Ian Goodfellow et al.](https://www.deeplearningbook.org/contents/ml.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es sei gesagt, dass sich das hier gezeigte Modell durch den Einsatz von größeren Datenmengen sehr stark verbessern würde. Dies hat vorwiegend zwei Gründe:\n",
    "* Deep Learning-Modelle skalieren linear mit mehr Daten (gute Intuition von [Kilian Weinberger](https://youtu.be/kPXxbmBsFxs?t=579))\n",
    "* Mehr Daten haben eine Vergrößerung des Vokabulars zur Folge. Dies reduziert die Verwendung des Out-of-Vocabulary-Tokens in vorher unbekannten Daten und erhöht die Genauigkeit\n",
    "\n",
    "Bei der Größe der vorliegenden Daten wäre zu prüfen, ob sich klassische ML-Ansätze nicht sogar besser eignen würden. Dazu zählen unter anderem das [Naive Bayes-Modell](https://sebastianraschka.com/Articles/2014_naive_bayes_1.html). Im Rahmen dieser Projektvorstellung sollte jedoch verdeutlicht werden, dass Deep Learning-Modelle (in begrenztem Rahmen) schon mit wenigen Daten sinnvolle Ergebnisse erzielen können. Dies trifft sogar noch mehr zu, wenn moderne NLP-Ansätze mit *vortrainierten* neuronalen Netzen eingesetzt werden (wie beispielsweise in diesem [Projekt](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Aus CRISP-DM 1.0:** *Die Erstellung des Modells bedeutet im Allgemeinen nicht das Ende des Projekts. Auch wenn der Zweck des Modells darin besteht, das Wissen über die Daten zu erweitern, müssen die gewonnenen Erkenntnisse so organisiert und präsentiert werden, dass der Kunde sie nutzen kann. Dies beinhaltet oft die Anwendung von \"Live\"-Modellen innerhalb der Entscheidungsfindungsprozesse einer Organisation - zum Beispiel in der Echtzeit-Personalisierung von Webseiten oder der wiederholten Bewertung von Marketingdatenbanken. Je nach den Anforderungen kann die Deployment Phase so einfach sein wie die Erstellung eines Berichts oder so komplex wie die Implementierung eines wiederholbaren Data Mining-Prozesses im gesamten Unternehmen. In vielen Fällen ist es der Kunde, der die Bereitstellung durchführt und nicht der Data Scientist. Aber selbst wenn der Data Scientist die Bereitstellung durchführt, ist es für den Kunden wichtig zu verstehen, welche Aktionen durchgeführt werden müssen, um die erstellten Modelle tatsächlich zu nutzen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dieser Phase soll das Modell (der Chatbot) in die Anwendung gebracht, also *ausgerollt* werden. Für dieses Beispielprojekt könnte man sich die Integration in das Enterprise-System eines Krankhauses vorstellen, in welchem der Bot ebenfalls API-Aufrufe absetzen kann und Mitarbeiter durchs Programm begleitet. Der Bot könnte auch per API (z.B. REST) zugänglich gemacht werden oder als Webservice laufen. Die Umsetzung dieser Use Cases sprengt allerdings den Rahmen dieser Projektvorstellung und bleibt den Teilnehmern überlassen. Es sei jedoch gesagt, dass alle Tensorflow-Modelle gespeichert, exportiert und portiert werden können. Nähere Informationen findet man unter [Tensorflow TFX](https://www.tensorflow.org/tfx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein wenig \"Deployment\" gibt es dann doch: der Chatbot kann innerhalb dieses Notebooks selbst ausprobiert werden. Der Bot wählt dabei zufällig eine Antwortmöglichkeit aus den Responses aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    print(\"Start talking with the bot (type \\\"quit\\\" to stop)!\")\n",
    "    print('\\nBot: Hi! What can I do for you?')\n",
    "    while True:\n",
    "        inp = input(\"Sie: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "            \n",
    "        sequence = tokenizer.texts_to_sequences([inp])\n",
    "        padded_sequence = pad_sequences(sequence, maxlen=padded_length)\n",
    "        prediction = model.predict(padded_sequence)[0]\n",
    "        pred_class_coded = np.argmax(prediction)\n",
    "        pred_class_explicit = tag_list[pred_class_coded]\n",
    "        intent = intents['intents'][pred_class_coded]\n",
    "        random_answer = rand.choice(intent['responses'])\n",
    "        \n",
    "        print(f\"Bot: To your {pred_class_explicit} I say {random_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Ausprobieren einfach folgenden Funktionsaufruf auskommentieren. Viel Spass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wie geht es weiter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mögliche Verbesserungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das hier vorgestellte Deep Learning-Modell soll als Einstieg in die Thematik dienen und ist daher eher als ein *Grundgerüst* zu verstehen, welches noch an einigen Stellen verbessert werden kann. Diese Verbesserungen obliegen den Teilnehmern, wobei in diesem Abschnitt diverse Verbesserungsansätze (ohne Anspruch auf Vollständigkeit) aufgezeigt werden sollen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Anpassung von Hyperparametern:** Die Hyperparameter sind jene Parameter, die das Netzwerk nicht selbst während des Trainings anpasst (*lernt*), sondern die vom Anwender vor Trainingsbeginn gesetzt werden. Dazu gehört beispielsweise die *Lernrate*, die *Batchsize*, die Anzahl der *Epochen*, die Art des *Optimierers* und die Architektur des Netzwerks selbst, sowie viele Weitere. Die Werte dieser Parameter haben einen monumentalen Effekt auf das Training des neuronalen Netzes und bilden oft ein instabiles Konstrukt, wodurch kleine Änderungen oft schon große Auswirkungen auf die Effektivität des resultierenden Netwerks haben können. Die Einstellung der Hyperparameter erfordert daher zumindest ein grundlegendes Verständnis über deren Wirkungsweise. Die Anpassung kann dann in einem *Trial-and-Error*-Verfahren erfolgen. Eine elegantere Herangehensweise ist die Verwendung von (stochastischen) Suchalgorithmen, welche in einem gegebenen Hyperparameterraum das beste Modell finden. Eine einfache Implementierung solcher Algorithmen findet sich u.a. in der `keras-tuner`-[Bibliothek](https://keras-team.github.io/keras-tuner/).\n",
    "* **Verwendung eines anderen Modells:** Das in diesem Notebook gezeigte Deep Learning-Modell ist als Feedforward-Modell denkbar einfach und entspricht leider nicht mehr dem aktuellen Stand der Wissenschaft (und Praxis!) im NLP-Bereich. Für eine Textklassifikationsaufgabe würde zumindest ein *Recurrent Neural Network* zum Einsatz kommen, welches durch seine spezielle Modellarchitektur in der Lage ist, eine Sequenz von Ereignissen zu modellieren. Auf die Aufgabenstellung bezogen bedeutet das, ein RNN könnte erkennen, in welcher Reihenfolge die Worte angeordnet sind und daher eine feinere Klassifikation erzielen. Für eine Einführung in RNNs und warum diese so effektiv sind, wird dem geneigten Leser unbedingt ein fantastischer Blogpost von [Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) ans Herz gelegt. Für Hinweise über eine Umsetzung in Tensorflow ist dieses [Tutorial](https://www.tensorflow.org/tutorials/text/text_classification_rnn) hilfreich.\n",
    "* **Verwendung vortrainierter Word-Embeddings:** Die Word-Embeddings unseres Modells wurden mitsamt des Modells trainiert und erlernen die Bedeutung von Wörtern anhand der Trainingsdaten. Gerade weil diese Datenmenge aber stark begrenzt ist, könnte es sinnvoll sein, bereits trainierte Word-Embeddings zu verwenden. Diese wurden an extrem umfassenden Textkorpussen erprobt (mehrere GB an Textdaten). Dies hat den Vorteil, dass mit Wortähnlichkeiten gearbeitet werden kann, welche in den Trainingsdaten nicht vorkommen. Wird der Chatbot beispielsweise anstatt nach einer \"Pharmacy\" nach einer \"Apothecary\" gefragt (dieses Wort kommt nicht in den Trainingsdaten vor), kann durch die vortrainierten Word-Embeddings trotzdem eine Ähnlichkeit hergestellt werden und daher eine Klassifikation erfolgen. Bekannte Word-Embeddings sind beispielsweise [GloVe](https://nlp.stanford.edu/projects/glove/) und [Word2vec](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "* **Mehr Daten sammeln:** Wie bereits im Abschnitt *Evaluation* erwähnt, könnte das hier gezeigte Modell deutlich von einer größeren Datenmenge profitieren. Dies gilt für Machine Learning-Modelle im Allgemeinen, so dass die Sammlung und Verarbeitung von qualitativ hochwertigen Daten vermutlich einen Großteil des Projektes ausmacht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für alle Anpassungen des Modells muss allerdings stets ein Grundsatz beachtet werden, welcher als ein **zentrales Problem des Machine Learning** verstanden werden kann: **Underfitting vs Overfitting!** Dieser [Blogpost](https://www.analyticsvidhya.com/blog/2020/02/underfitting-overfitting-best-fitting-machine-learning/) gibt einen guten, nicht-technischen Überblick über das Problem. \n",
    "\n",
    "Intuitiv wollen wir ein Deep Learning-Modell erhalten, welches durch das Trainingsprozedere ein *Verständnis* für die zugrundeliegenden Daten erreicht und dieses Verständnis auf neue Daten übertragen kann. Problematisch wird es, wenn das Modell: A) dieses Verständnis nie erreicht, da das Problem für das verwendete Modell zu komplex ist oder B) das Modell die Trainingsdaten lediglich *auswendig* lernt und damit die Fähigkeit zur *Generalisierung* neuer Daten verliert. Im Falle A) spricht man von *Underfitting* und bei B) handelt es sich um *Overfitting*. Die *Kapazität* des Modells, also dessen Potential mit komplexen Datenstrukturen umzugehen, muss immer im Verhältnis zu den vorliegenden Daten stehen. Die beiden Regime sollen durch folgende Abbildung verdeutlicht werden:\n",
    "\n",
    "![Kapazität](./images/capacity_s.png \"Kapazität\")\n",
    "\n",
    "In der Praxis existieren daher handfeste Strategien die Kapazität eines Modells anzupassen und damit Overfitting bzw. Underfitting zu vermeiden. Dieses Tensorflow [Tutorial](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit) gibt beispielsweise einen guten Einstieg. Wer sich näher mit der Theorie der Modellkapazität beschäftigen möchte, dem wird erneut Kapitel 5.2 von [Ian Goodfellow et al.](https://www.deeplearningbook.org/contents/ml.html) ans Herz gelegt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausblick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Rahmen des Projektes soll ein Deep Learning-Modell in die Praxis überführt werden. Anders ausgedrückt: das Modell soll in einem Frontend zum Einsatz kommen. Dafür müssen die Themen *Export* und *Deployment/Implementierung* geklärt werden. Eine sinnvolle Herangehensweise soll von den Teilnehmern erarbeitet werden. Bei der Verwendung von Tensorflow sollte gegebenenfalls [Tensorflow TFX](https://www.tensorflow.org/tfx) untersucht werden, wobei unzählige weitere Möglichkeiten der Bereitstellung bestehen.\n",
    "\n",
    "Abseits von der in diesem Notebook dargestellten Aufgabe der *Textklassifikation* gibt es noch weitere Deep Learning NLP-Anwendungen. Einige der spannendsten sind:\n",
    "* **Text Generation:** Wäre es nicht angebracht, wenn das Modell *intelligent* genug wäre, eine eigene Antwort zu formulieren, anstatt eine vorgegebene Antwort zufällig auszuwählen? Mit moderneren NN-Architekturen lässt sich dies umsetzen. Ein Beispielprojekt eines solchen *Conversational Chatbots* gibt es beispielsweise von [Thomas Wolf](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313). Auf [Talk to Transformer](https://talktotransformer.com/) kann man einem solchen Netzwerk interaktiv auf den Zahn fühlen. Die Ergebnisse sind oft überraschend gut!\n",
    "* **Neural Machine Translation:** Moderne Encoder-Decoder-Architekturen ermöglichen eine recht präzise, automatische Übersetzung von einer Sprache in eine andere. Viele Services (wie z.B. [DeepL](https://www.deepl.com/home)) nutzen solche Architekturen bereits. Eine Einführung in die Thematik gibt beispielsweise [Jason Brownlee](https://machinelearningmastery.com/introduction-neural-machine-translation/). Für Ansatzpunkte einer technischen Umsetzung könnte dieses [Tensorflow Tutorial](https://www.tensorflow.org/tutorials/text/nmt_with_attention) relevant sein.\n",
    "* **Text Summarization:** Text Summarization ist ein weiteres Anwendungsgebiet des NLP. Wie der Name schon vermuten lässt, geht es um die automatische Extraktion der Kerninhalte eines Textes, welche in einer *Summary* zusammengefasst werden. Dieser [Blogpost](https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/) erläutert die konzeptionellen Aspekte und gibt sogar Ansatzpunkte für eine Umsetzung in Keras/Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
